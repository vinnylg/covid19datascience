{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set covid19_datasciente as path for python find bulletin package\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from os import getcwd\n",
    "sys.path.append(str(Path(getcwd()).parent.parent)) ## ../../covid19datascience* <- set parent level here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from epiweeks import Week\n",
    "from datetime import timedelta, date\n",
    "from os.path import join\n",
    "from unidecode import unidecode\n",
    "\n",
    "from bulletin import default_input, default_output, root, hoje, data_comeco_pandemia\n",
    "from bulletin.utils.normalize import normalize_text, normalize_hash\n",
    "from bulletin.systems.sim import Sim\n",
    "from bulletin.systems.sivep import Sivep\n",
    "from bulletin.systems.notifica import Notifica\n",
    "from bulletin.systems.casos_confirmados import CasosConfirmados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import binascii\n",
    "from bisect import bisect_right\n",
    "from heapq import heappop, heappush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_plagiaries(dict, df):\n",
    "    # Map the two documents to each other.\n",
    "    dict[df.index.values[0]] = df['id_duplicado'].values[0]\n",
    "    dict[df['id_duplicado'].values[0]] = df.index.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CasosConfirmados()\n",
    "cc.load('cc_19_08_2021')\n",
    "covid_obitos = cc.df.loc[cc.df['evolucao']==2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim = Sim()\n",
    "# sim.load('dopr')\n",
    "#sim.read_all_database_files()\n",
    "#sim.fix_dtypes()\n",
    "#sim.save(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim.df = sim.df.loc[\n",
    "#     sim.df['CAUSABAS'].str.contains('B342') |\n",
    "#     sim.df['LINHAA'].str.contains('B342') |\n",
    "#     sim.df['LINHAB'].str.contains('B342') |\n",
    "#     sim.df['LINHAC'].str.contains('B342') |\n",
    "#     sim.df['LINHAC'].str.contains('B342') |\n",
    "#     sim.df['LINHAII'].str.contains('B342')\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim.to_notifica()\n",
    "# sim.hashes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_covid_obitos = sim.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_covid_obitos = sim_covid_obitos.loc[sim_covid_obitos['ibge_residencia']=='410690'].copy()\n",
    "#sim_covid_obitos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_covid_obitos = sim_covid_obitos.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ident in sim_covid_obitos.loc[sim_covid_obitos['hash_mae'].notna() & sim_covid_obitos.duplicated('hash_mae',keep='first')]['id'].values:\n",
    "#     sim_covid_obitos.loc[sim_covid_obitos['id']==ident, 'id_duplicado'] = sim_covid_obitos.loc[(sim_covid_obitos['hash_mae']==sim_covid_obitos.loc[sim_covid_obitos['id']==ident]['hash_mae'].values[0]) & (sim_covid_obitos.duplicated('hash_mae',keep='last'))]['id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_covid_obitos.loc[sim_covid_obitos['id_duplicado'].notna()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sivep = Sivep()\n",
    "# sivep.load('sraghospitalizado')\n",
    "# sivep.read_all_database_files()\n",
    "# sivep.fix_dtypes()\n",
    "# sivep.save(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sivep.to_notifica()\n",
    "# sivep.hashes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sivep_covid_obitos = sivep.get_obitos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sivep_covid_obitos = sivep_covid_obitos.loc[sivep_covid_obitos['ibge_residencia']=='411520.0']\n",
    "# sivep_covid_obitos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ident in covid_obitos.loc[covid_obitos['hash_mae'].notna() & covid_obitos.duplicated('hash_mae',keep='first')]['id'].values:\n",
    "    covid_obitos.loc[covid_obitos['id']==ident, 'id_duplicado'] = covid_obitos.loc[(covid_obitos['hash_mae']==covid_obitos.loc[covid_obitos['id']==ident]['hash_mae'].values[0]) & (covid_obitos.duplicated('hash_mae',keep='last'))]['id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_obitos.loc[covid_obitos['id_duplicado'].notna()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ident in sivep_covid_obitos.loc[sivep_covid_obitos['hash_mae'].notna() & sivep_covid_obitos.duplicated('hash_mae',keep='first')]['id'].values:\n",
    "#     sivep_covid_obitos.loc[sivep_covid_obitos['id']==ident, 'id_duplicado'] = sivep_covid_obitos.loc[(sivep_covid_obitos['hash_mae']==sivep_covid_obitos.loc[sivep_covid_obitos['id']==ident]['hash_mae'].values[0]) & (sivep_covid_obitos.duplicated('hash_mae',keep='last'))]['id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sivep_covid_obitos.loc[sivep_covid_obitos['id_duplicado'].notna()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del sivep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interseccao_hash = sim_covid.loc[\n",
    "#     sim_covid['hash_mae'].notna() & sim_covid['hash_mae'].isin(sivep_covid['hash_mae'])\n",
    "# ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Óbitos confirmados SIVEP: {sivep_covid.shape[0]}\")\n",
    "# print(f\"Óbitos com menção ao covid SIM: {sim_covid.shape[0]}\")\n",
    "# print(f\"Intersecção das duas bases de dados usando hash: {interseccao_hash.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataFile = sim_covid.loc[(~sim_covid['hash_mae'].isnull())].copy()\n",
    "# truthFile = sivep_covid.loc[(~sivep_covid['hash_mae'].isnull())].drop_duplicates('hash_mae', keep = 'last').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#truthFile.loc[truthFile['hash_mae'].isin(dataFile['hash_mae']), 'id_in_sim'] = dataFile.loc[dataFile['hash_mae'].isin(truthFile['hash_mae']), 'id']\n",
    "#del truthFile['id_in_sim']\n",
    "# for mae in truthFile['hash_mae'].values:\n",
    "#     if (mae in dataFile['hash_mae'].values):\n",
    "#         truthFile.loc[truthFile['hash_mae']==mae, 'id_in_sim'] = dataFile.loc[dataFile['hash_mae']==mae]['id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = covid_obitos.loc[covid_obitos['hash_mae'].notna()]\n",
    "truthFile = covid_obitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataFile = sivep_covid_obitos.loc[sivep_covid_obitos['hash_mae'].notna()]\n",
    "# truthFile = sivep_covid_obitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataFile = sim_covid_obitos.loc[sim_covid_obitos['hash_mae'].notna()]\n",
    "# truthFile = sim_covid_obitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = dataFile.set_index('id')\n",
    "truthFile = truthFile.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del sim_covid\n",
    "# del sivep_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numHashes = 10\n",
    "numDocs = dataFile.shape[0]# if (dataFile.shape[0] > truthFile.shape[0]) else truthFile.shape[0]\n",
    "print([numDocs, numHashes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truthFile.loc[truthFile['id_in_sim'].notna()]\n",
    "#truthFile.loc[truthFile.index=='315849756828']\n",
    "#dataFile.loc[dataFile.index=='285711644']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#                  Parse The Ground Truth Tables\n",
    "# =============================================================================\n",
    "# Build a dictionary mapping the document IDs to their plagiaries, and vice-\n",
    "# versa.\n",
    "plagiaries = {}\n",
    "\n",
    "for idx in truthFile.loc[truthFile['id_duplicado'].notna()].index:\n",
    "    match_plagiaries(plagiaries,truthFile.loc[truthFile.index==idx])\n",
    "plagiaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#               Convert Documents To Sets of Shingles\n",
    "# =============================================================================\n",
    "\n",
    "# The current shingle ID value to assign to the next new shingle we \n",
    "# encounter. When a shingle gets added to the dictionary, we'll increment this\n",
    "# value.\n",
    "curShingleID = 0\n",
    "\n",
    "# Create a dictionary of the articles, mapping the article identifier (e.g., \n",
    "# \"t8470\") to the list of shingle IDs that appear in the document.\n",
    "docsAsShingleSets = {}\n",
    "\n",
    "docNames = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "totalShingles = 0\n",
    "\n",
    "k = range(3)\n",
    "\n",
    "for i in tqdm(dataFile.index):\n",
    "  \n",
    "  # Retrieve the article ID, which is the first word on the line.  \n",
    "  docID = i\n",
    "  \n",
    "  # Maintain a list of all document IDs.  \n",
    "  docNames.append(docID)\n",
    "  \n",
    "  # 'shinglesInDoc' will hold all of the unique shingle IDs present in the \n",
    "  # current document. If a shingle ID occurs multiple times in the document,\n",
    "  # it will only appear once in the set (this is a property of Python sets).\n",
    "  shinglesInDoc = set()\n",
    "  \n",
    "  # For each word in the document...\n",
    "  for index in range(0, len(dataFile['hash_mae'][i]) - 2):\n",
    "    # Construct the shingle text by combining three words together.\n",
    "    shingle = ''\n",
    "    for j in k:\n",
    "      shingle = shingle+dataFile['hash_mae'][i][index+j]\n",
    "\n",
    "    # Hash the shingle to a 32-bit integer.\n",
    "    crc = binascii.crc32(bytearray(shingle, \"utf8\")) & 0xffffffff\n",
    "    \n",
    "    # Add the hash value to the list of shingles for the current document. \n",
    "    # Note that set objects will only add the value to the set if the set \n",
    "    # doesn't already contain it. \n",
    "    shinglesInDoc.add(crc)\n",
    "  \n",
    "  # Store the completed list of shingles for this document in the dictionary.\n",
    "  docsAsShingleSets[docID] = shinglesInDoc\n",
    "  \n",
    "  # Count the number of shingles across all documents.\n",
    "  totalShingles = totalShingles + (len(dataFile['hash_mae'][i]) - 2)\n",
    "\n",
    "# Report how long shingling took.\n",
    "print('\\nShingling ' + str(numDocs) + ' docs took %.2f sec.' % (time.time() - t0))\n",
    " \n",
    "print('\\nAverage shingles per doc: %.2f' % (totalShingles / numDocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#                     Define Triangle Matrices\n",
    "# =============================================================================\n",
    "\n",
    "# Define virtual Triangle matrices to hold the similarity values. For storing\n",
    "# similarities between pairs, we only need roughly half the elements of a full\n",
    "# matrix. Using a triangle matrix requires less than half the memory of a full\n",
    "# matrix, and can protect the programmer from inadvertently accessing one of\n",
    "# the empty/invalid cells of a full matrix.\n",
    "\n",
    "# Calculate the number of elements needed in our triangle matrix\n",
    "numElems = int(numDocs * (numDocs - 1) / 2)\n",
    "\n",
    "# Initialize two empty lists to store the similarity values. \n",
    "# 'JSim' will be for the actual Jaccard Similarity values. \n",
    "# 'estJSim' will be for the estimated Jaccard Similarities found by comparing\n",
    "# the MinHash signatures.\n",
    "JSim = [0 for x in range(numElems)]\n",
    "estJSim = [0 for x in range(numElems)]\n",
    "\n",
    "# Define a function to map a 2D matrix coordinate into a 1D index.\n",
    "def getTriangleIndex(i, j):\n",
    "  # If i == j that's an error.\n",
    "  if i == j:\n",
    "    sys.stderr.write(\"Can't access triangle matrix with i == j\")\n",
    "    sys.exit(1)\n",
    "  # If j < i just swap the values.\n",
    "  if j < i:\n",
    "    temp = i\n",
    "    i = j\n",
    "    j = temp\n",
    "  \n",
    "  # Calculate the index within the triangular array.\n",
    "  # This fancy indexing scheme is taken from pg. 211 of:\n",
    "  # http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n",
    "  # But I adapted it for a 0-based index.\n",
    "  # Note: The division by two should not truncate, it\n",
    "  #       needs to be a float. \n",
    "  k = int(i * (numDocs - (i + 1) / 2.0) + j - i) - 1\n",
    "  \n",
    "  return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#                 Generate MinHash Signatures\n",
    "# =============================================================================\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "print('\\nGenerating random hash functions...')\n",
    "\n",
    "# Record the maximum shingle ID that we assigned.\n",
    "maxShingleID = 2**32-1\n",
    "\n",
    "# We need the next largest prime number above 'maxShingleID'.\n",
    "# I looked this value up here: \n",
    "# http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php\n",
    "nextPrime = 4294967311\n",
    "\n",
    "\n",
    "# Our random hash function will take the form of:\n",
    "#   h(x) = (a*x + b) % c\n",
    "# Where 'x' is the input value, 'a' and 'b' are random coefficients, and 'c' is\n",
    "# a prime number just greater than maxShingleID.\n",
    "\n",
    "# Generate a list of 'k' random coefficients for the random hash functions,\n",
    "# while ensuring that the same value does not appear multiple times in the \n",
    "# list.\n",
    "def pickRandomCoeffs(k):\n",
    "  # Create a list of 'k' random values.\n",
    "  randList = []\n",
    "  \n",
    "  while k > 0:\n",
    "    # Get a random shingle ID.\n",
    "    randIndex = random.randint(0, maxShingleID) \n",
    "  \n",
    "    # Ensure that each random number is unique.\n",
    "    while randIndex in randList:\n",
    "      randIndex = random.randint(0, maxShingleID) \n",
    "    \n",
    "    # Add the random number to the list.\n",
    "    randList.append(randIndex)\n",
    "    k = k - 1\n",
    "    \n",
    "  return randList\n",
    "\n",
    "# For each of the 'numHashes' hash functions, generate a different coefficient 'a' and 'b'.   \n",
    "coeffA = pickRandomCoeffs(numHashes)\n",
    "coeffB = pickRandomCoeffs(numHashes)\n",
    "\n",
    "print('\\nGenerating MinHash signatures for all documents...')\n",
    "\n",
    "# List of documents represented as signature vectors\n",
    "signatures = []\n",
    "\n",
    "# Rather than generating a random permutation of all possible shingles, \n",
    "# we'll just hash the IDs of the shingles that are *actually in the document*,\n",
    "# then take the lowest resulting hash code value. This corresponds to the index \n",
    "# of the first shingle that you would have encountered in the random order.\n",
    "\n",
    "# For each document...\n",
    "for docID in docNames:\n",
    "  \n",
    "  # Get the shingle set for this document.\n",
    "  shingleIDSet = docsAsShingleSets[docID]\n",
    "  \n",
    "  # The resulting minhash signature for this document. \n",
    "  signature = []\n",
    "  \n",
    "  # For each of the random hash functions...\n",
    "  for i in range(0, numHashes):\n",
    "    \n",
    "    # For each of the shingles actually in the document, calculate its hash code\n",
    "    # using hash function 'i'. \n",
    "    \n",
    "    # Track the lowest hash ID seen. Initialize 'minHashCode' to be greater than\n",
    "    # the maximum possible value output by the hash.\n",
    "    minHashCode = nextPrime + 1\n",
    "    \n",
    "    # For each shingle in the document...\n",
    "    for shingleID in shingleIDSet:\n",
    "      # Evaluate the hash function.\n",
    "      hashCode = (coeffA[i] * shingleID + coeffB[i]) % nextPrime \n",
    "      \n",
    "      # Track the lowest hash code seen.\n",
    "      if hashCode < minHashCode:\n",
    "        minHashCode = hashCode\n",
    "\n",
    "    # Add the smallest hash code value as component number 'i' of the signature.\n",
    "    signature.append(minHashCode)\n",
    "  \n",
    "  # Store the MinHash signature for this document.\n",
    "  signatures.append(signature)\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "        \n",
    "print(\"\\nGenerating MinHash signatures took %.2fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#                 Calculate Jaccard Similarities\n",
    "# =============================================================================\n",
    "# In this section, we will directly calculate the Jaccard similarities by \n",
    "# comparing the sets. This is included here to show how much slower it is than\n",
    "# the MinHash approach.\n",
    "\n",
    "# Calculating the Jaccard similarities gets really slow for large numbers\n",
    "# of documents.\n",
    "if numDocs <= 2500:\n",
    "#if True:\n",
    "    print(\"\\nCalculating Jaccard Similarities...\")\n",
    "\n",
    "    # Time the calculation.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # For every document pair...\n",
    "    for i in range(0, numDocs):\n",
    "      \n",
    "      # Print progress every 100 documents.\n",
    "      if (i % 100) == 0:\n",
    "        print(\"  (\" + str(i) + \" / \" + str(numDocs) + \")\")\n",
    "\n",
    "      # Retrieve the set of shingles for document i.\n",
    "      s1 = docsAsShingleSets[docNames[i]]\n",
    "      \n",
    "      for j in range(i + 1, numDocs):\n",
    "        # Retrieve the set of shingles for document j.\n",
    "        s2 = docsAsShingleSets[docNames[j]]\n",
    "        \n",
    "        # Calculate and store the actual Jaccard similarity.\n",
    "        JSim[getTriangleIndex(i, j)] = (len(s1.intersection(s2)) / len(s1.union(s2)))    \n",
    "\n",
    "    # Calculate the elapsed time (in seconds)\n",
    "    elapsed = (time.time() - t0)\n",
    "        \n",
    "    print(\"\\nCalculating all Jaccard Similarities took %.2fsec\" % elapsed)\n",
    "\n",
    "# Delete the Jaccard Similarities, since it's a pretty big matrix.    \n",
    "del JSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#                     Compare All Signatures\n",
    "# =============================================================================  \n",
    "  \n",
    "# Creates a N x N matrix initialized to 0.\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# For each of the test documents...\n",
    "for i in tqdm(range(0, numDocs)):\n",
    "  # Get the MinHash signature for document i.\n",
    "  signature1 = signatures[i]\n",
    "    \n",
    "  # For each of the other test documents...\n",
    "  for j in range(i + 1, numDocs):\n",
    "    \n",
    "    # Get the MinHash signature for document j.\n",
    "    signature2 = signatures[j]\n",
    "    \n",
    "    count = 0\n",
    "    # Count the number of positions in the minhash signature which are equal.\n",
    "    for k in range(0, numHashes):\n",
    "      count = count + (signature1[k] == signature2[k])\n",
    "    \n",
    "    # Record the percentage of positions which matched.    \n",
    "    estJSim[getTriangleIndex(i, j)] = (count / numHashes)\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "        \n",
    "print(\"\\nComparing MinHash signatures took %.2fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#                   Display Similar Document Pairs\n",
    "# =============================================================================  \n",
    "\n",
    "# Count the true positives and false positives.\n",
    "tp = 0\n",
    "fp = 0\n",
    "  \n",
    "threshold = 0.8\n",
    "print(\"\\nList of Document Pairs with J(d1,d2) more than\", threshold)\n",
    "print(\"Values shown are the estimated Jaccard similarity and the actual\")\n",
    "print(\"Jaccard similarity.\\n\")\n",
    "print(\"                   Est. J   Act. J\")\n",
    "\n",
    "# For each of the document pairs...\n",
    "for i in range(0, numDocs):  \n",
    "  for j in range(i + 1, numDocs):\n",
    "    # Retrieve the estimated similarity value for this pair.\n",
    "    estJ = estJSim[getTriangleIndex(i, j)]\n",
    "    \n",
    "    # If the similarity is above the threshold...\n",
    "    if estJ > threshold:\n",
    "    \n",
    "      # Calculate the actual Jaccard similarity for validation.\n",
    "      s1 = docsAsShingleSets[docNames[i]]\n",
    "      s2 = docsAsShingleSets[docNames[j]]\n",
    "      J = 0 if (len(s1.union(s2))==0) else (len(s1.intersection(s2)) / len(s1.union(s2)))\n",
    "      \n",
    "      # Print out the match and similarity values with pretty spacing.\n",
    "      #print(\"  %5s --> %5s   %.2f     %.2f\" % (docNames[i], docNames[j], estJ, J))\n",
    "      \n",
    "      # Check whether this is a true positive or false positive.\n",
    "      # We don't need to worry about counting the same true positive twice\n",
    "      # because we implemented the for-loops to only compare each pair once.\n",
    "      try:\n",
    "          if plagiaries[docNames[i]] == docNames[j]:\n",
    "              tp = tp + 1\n",
    "              print(\"tp:  %5s --> %5s   %.2f     %.2f\" % (docNames[i], docNames[j], estJ, J))\n",
    "              dataFile.loc[[docNames[i]], 'id_duplicado'] = dataFile.loc[[docNames[j]]].index.values[0]\n",
    "              dataFile.loc[[docNames[j]], 'id_duplicado'] = dataFile.loc[[docNames[i]]].index.values[0]\n",
    "          else:\n",
    "              print(\"fp:  %5s --> %5s   %.2f     %.2f\" % (docNames[i], docNames[j], estJ, J))\n",
    "              dataFile.loc[[docNames[i]], 'id_duplicado'] = dataFile.loc[[docNames[j]]].index.values[0]\n",
    "              dataFile.loc[[docNames[j]], 'id_duplicado'] = dataFile.loc[[docNames[i]]].index.values[0]\n",
    "              fp = fp + 1\n",
    "      except:\n",
    "          pass\n",
    "\n",
    "\n",
    "# Display true positive and false positive counts.\n",
    "print()\n",
    "print(\"True positives:  \" + str(tp) + \" / \" + str(int(len(plagiaries.keys()) / 2)))\n",
    "print(\"False positives: \" + str(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CasosConfirmados()\n",
    "cc.df = dataFile\n",
    "cc.save('obitos_minhash')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
